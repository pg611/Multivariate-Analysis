---
title: "Layoff_Final_Analysis"
author: "pg611@scarletmail.rutgers.edu"
date: "2024-04-29"
output: html_document
---
```{r}
library(MASS)
library(ggplot2)
library(memisc)
library(ROCR)
library(dplyr)
library(klaR)
library(memisc)
library(ggplot2)
library(corrplot)
library(factoextra)
library(psych)
library(NbClust)
library(caTools)
library(magrittr)
library(pROC)
library(caret)


sc_dataset <- read.csv("/Users/parul/OneDrive/Desktop/MVA/Layoff_Data.csv", row.names=1)
str(sc_dataset)

```

# DATA COLLECTION :

* Source: Kaggle
* Dataset Name: Layoffs Dataset
* URL: https://www.kaggle.com/datasets/swaptr/layoffs-2022
* Description: The dataset was retrieved from Kaggle, a platform known for hosting datasets and machine learning competitions. It contains information on layoffs within the technology industry, including various attributes such as company name, number of layoffs, percentage of employees laid off, financial metrics, geographic location, and more.

# DATA DICTIONARY :

* Company: Name of the technology company.
* Laid_Off: Number of employees laid off.
* Percentage: Percentage of employees laid off.
* Before_Layoffs: Number of employees before the layoffs.
* After_layoffs: Number of employees after the layoffs.
* Money_Raised_in_$_mil: Amount of money raised by the company in millions (in $).
* lat: Latitude coordinate of the company's location.
* lng: Longitude coordinate of the company's location.
* layoff: Binary indicator of layoffs (1 for layoffs, 0 for no layoffs).
* Laid_Off_Yes_No: Categorical indicator of layoffs (Yes for layoffs, No for no layoffs).


# QUESTIONS and HYPOTHESIS Questions :

* What are the key factors contributing to layoffs in the technology industry?

* How accurately can we predict the likelihood of future layoffs in tech companies based on historical data?


# Analysis and Visualization :

```{r }
str(sc_dataset)
summary(sc_dataset)
stars(sc_dataset)
```


```{r}
correlation_matrix <- cor(sc_dataset[,1:8])
correlation_matrix

corrplot(cor(sc_dataset[,1:8]), type = "upper", method = "color")


```
• The correlation matrix shows us a correlation between the columns in both cases. 

• Hence, Principal Component Analysis (PCA) can be used to reduce the number of columns for the analysis. 

# PCA :

```{r}
# PCA

sc_dataset_pca <- prcomp(sc_dataset[,1:7],scale=TRUE) 
sc_dataset_pca

summary(sc_dataset_pca)

fviz_eig(sc_dataset_pca, addlabels = TRUE)
#plot(sc_dataset, xlab = "Component number", ylab = "Component variance", main = "Scree diagram")

```

• The scree diagram shows us that sum of the first 2 principal components is less than 70%.

• So, we cannot move forward using PCA for column reduction. 

• We now move on to check EFA for this dataset.

```{r}

fviz_pca_var(sc_dataset_pca,col.var = "cos2",
             gradient.cols = c("#FFCC00", "#CC9933", "#660033", "#330033"),
             repel = TRUE)
```


# FACTOR ANALYSIS :

```{r}
# EFA


fit.pc <- principal(sc_dataset[,1:7], nfactors=4, rotate="varimax") 
fit.pc

fa.diagram(fit.pc) 

vss(sc_dataset[,1:7])

```

Defining the factors obtained :

* RC1: Layoff Metrics

RC1 focuses on essential metrics directly related to layoffs within tech companies. "Before_Layoffs" represents the number of employees before layoffs, "After_Layoffs" indicates the number of employees after layoffs occurred, and "Laid_Off" signifies the actual number of employees laid off. This RCS provides crucial information about the scale and impact of layoffs within the dataset.

* RC2: Geospatial Coordinates

RC2 encompasses geospatial coordinates associated with the location of tech companies. "lat" denotes the latitude coordinate, and "lng" represents the longitude coordinate. This RCS facilitates geospatial analysis and visualization, allowing for insights into the geographic distribution and clustering of tech companies affected by layoffs.

* RC3: Funding Metrics


RC3 focuses on financial metrics related to funding raised by tech companies. "Money_Raised_in_$_mil" denotes the amount of money raised in millions of dollars. This RCS provides insights into the financial resources available to tech companies and their fundraising activities, which may influence their resilience to economic downturns and layoffs.

* RC4: Layoff Percentage

RC4 emphasizes the percentage of employees laid off as a proportion of the total workforce. "Percentage" represents the percentage of employees laid off relative to the total number of employees. This RCS offers insights into the severity of layoffs within tech companies and provides a standardized measure for comparing layoff impact across different organizations.

# CLUSTERING :
```{r}
# CLUSTERING

efa_data <- as.data.frame(fit.pc$scores)
efa_data


matstd_sm <- scale(efa_data)

#fviz_nbclust(matstd_sm, kmeans, method = "gap_stat")

res.hc <- matstd_sm %>% scale() %>% dist(method = "euclidean") %>%
  hclust(method = "ward.D2")

res.nbclust <- matstd_sm %>% scale() %>% NbClust(distance = "euclidean", min.nc = 2, max.nc = 4, method = "complete", index ="all") 

#fviz_nbclust(res.nbclust, ggtheme = theme_minimal())

km.res <- kmeans(matstd_sm, 2, nstart = 10)

fviz_cluster(km.res, data = matstd_sm,
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())

dist.sc_dataset <- dist(matstd_sm, method="euclidean")



fviz_dend(res.hc, k = 2, # Cut in four groups
          cex = 0.5, # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # Add rectangle around groups
)


Clustered <- ifelse(km.res$cluster > 1.5, "No layoff", "Layoff")
Actual <- ifelse(sc_dataset$layoff == 1, "Layoff", "No Layoff")
confusion_mat <- table(Clustered, Actual)
confusion_mat

accuracy <- sum(diag(confusion_mat)) / sum(confusion_mat)
precision <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
recall <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
cat("Accuracy:", round(accuracy, 3), "\n")

cat("Precision:", round(precision, 3), "\n")

cat("Recall:", round(recall, 3), "\n")




```
Clustering analysis has been conducted on the dataset, resulting in the formation of two distinct groups or clusters.


# REGRESSION :

```{r}
wdbc <- read.csv("/Users/parul/OneDrive/Desktop/MVA/Layoff_Data.csv")
features <- c("Laid_Off", "Percentage", "Before_Layoff", "After_Layoff", "Money_raised", "lat", "lng", "laid_yes_no", "layoff")
names(wdbc) <- c("Company", "Laid_Off", "Percentage", "Before_Layoff", "After_Layoff", "Money_raised", "lat", "lng", "laid_yes_no", "layoff")
wdbc<- wdbc
wdbc.data <- as.matrix(wdbc[,c(2:8)])
row.names(wdbc.data) <- wdbc$Company
wdbc_raw <- cbind(wdbc.data, as.numeric(as.factor(wdbc$laid_yes_no))-1)
colnames(wdbc_raw)[8] <- "LayoffYesNo"
smp_size_raw <- floor(0.70 * nrow(wdbc_raw))
train_ind_raw <- sample(nrow(wdbc_raw), size = smp_size_raw)
train_raw.df <- as.data.frame(wdbc_raw[train_ind_raw, ])
test_raw.df <- as.data.frame(wdbc_raw[-train_ind_raw, ])

```

Now divided the dataset into training and testing data for regression analysis.
We consider the training and testing data with a 70-30 split.



# MULTIPLE REGRESSION :

* Multiple regression is a statistical technique used to understand the relationship between one dependent variable and two or more independent variables. 

* It aims to predict the value of the dependent variable based on the values of the independent variables.
```{r}
# MULTIPLE REGRESSION
#fit <- lm(TroubleInSleep~Instagram + Snapchat + Twitter, data=sc_dataset) 
x_sm <- cbind(train_ind_raw,train_raw.df[, -9])

fit <- lm( LayoffYesNo~ ., data = train_raw.df) 

summary(fit)
coefficients(fit)
residuals(fit)
probabilities_sm2 <- predict(fit, newdata = test_raw.df, type = "response")

predicted_sm2 <- ifelse(probabilities_sm2 > 0.5, "Yes", "No")
actual_sm <- ifelse(test_raw.df$LayoffYesNo == 1, "Yes", "No")
confusion_sm2 <- table(predicted_sm2, actual_sm)
confusion_sm2

roc_sm <- roc(test_raw.df$LayoffYesNo, probabilities_sm2)
auc_sm <- auc(roc_sm)

ggroc(roc_sm, color = "blue", legacy.axes = TRUE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  scale_x_continuous(labels = scales::percent_format()) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "False Positive Rate", y = "True Positive Rate",
       title = paste("ROC Curve (AUC = ", round(auc_sm, 2), ")")) +
  annotate("text", x = 0.5, y = 0.5, label = paste0("AUC = ", round(auc_sm, 2)))

threshold<-0.5
predicted <- predict(fit, newdata = test_raw.df)

predicted_category <- ifelse(predicted > threshold, "No Layoff", "Layoff")

# Convert actual values to binary categories
actual_category <- ifelse(test_raw.df$LayoffYesNo == 1, "No Layoff", "Layoff")

# Create a confusion matrix
confusion_mat <- table(predicted_category, actual_category)

# Calculate accuracy
accuracy <- sum(diag(confusion_mat)) / sum(confusion_mat)
accuracy

```

The regression summary shows that we have significant variables that affect the output variable.

The regression summary shows that we have significant variables that affect the output variable.


# LOGISTIC REGRESSION :

* Logistic regression is a statistical method used for binary classification problems. It predicts the probability that a given observation belongs to one of two classes.
* It models the relationship between a binary dependent variable and one or more independent variables.

We can now check how the logistic regression-

```{r}
# Logistic Regression

logistic_sm <- glm(LayoffYesNo ~ ., data = train_raw.df, family = 'binomial')
summary(logistic_sm)

predicted.data <- data.frame(probability.of.hd=logistic_sm$fitted.values,LayoffYesNo=train_raw.df$LayoffYesNo)
predicted.data <- predicted.data[order(predicted.data$probability.of.hd, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)

ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) +
  geom_point(aes(color=LayoffYesNo), alpha=1, shape=4, stroke=2) +
  xlab("Index") +
  ylab("Predicted probability of getting in sleeping")
probabilities_sm2 <- predict(logistic_sm, newdata = test_raw.df, type = "response")

predicted_sm2 <- ifelse(probabilities_sm2 > 0.5, "Yes", "No")
actual_sm <- ifelse(test_raw.df$LayoffYesNo == 1, "Yes", "No")
confusion_sm2 <- table(predicted_sm2, actual_sm)
confusion_sm2
accuracy2 <- sum(diag(confusion_sm2)) / sum(confusion_sm2)
precision2 <- confusion_sm2[2, 2] / sum(confusion_sm2[, 2])
recall2 <- confusion_sm2[2, 2] / sum(confusion_sm2[2, ])
cat("Accuracy:", round(accuracy2, 3), "\n")
cat("Precision:", round(precision2, 3), "\n")
cat("Recall:", round(recall2, 3), "\n")
probabilities <- predict(logistic_sm, newdata = test_raw.df, type = "response")

roc_sm <- roc(test_raw.df$LayoffYesNo, probabilities_sm2)
auc_sm <- auc(roc_sm)

ggroc(roc_sm, color = "blue", legacy.axes = TRUE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  scale_x_continuous(labels = scales::percent_format()) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "False Positive Rate", y = "True Positive Rate",
       title = paste("ROC Curve (AUC = ", round(auc_sm, 2), ")")) +
  annotate("text", x = 0.5, y = 0.5, label = paste0("AUC = ", round(auc_sm, 2)))
```

These visualizations provide valuable insights into the logistic regression model's performance by showcasing Accuracy and ROC Curve of the model.



# DISCRIMINANT :

* Discriminant analysis is a statistical method used for classification tasks that involve identifying which category or group a new observation belongs to based on one or more predictor variables (features).

```{r}

# DISCRIMINANT MODEL
wdbc_raw.lda <- lda(formula = LayoffYesNo ~ ., data = train_raw.df)
wdbc_raw.lda
summary(wdbc_raw.lda)
plot(wdbc_raw.lda)
predictions <- predict(wdbc_raw.lda, newdata = test_raw.df)
predicted_classes <- predictions$class
accuracy2 <- mean(predicted_classes == test_raw.df$LayoffYesNo)
accuracy2
wdbc_raw.lda.predict_train <- predict(wdbc_raw.lda, newdata = train_raw.df)
y<-wdbc_raw.lda.predict_train$class
wdbc_raw.lda.predict_train$x
table(y,train_raw.df$LayoffYesNo)
wdbc_raw.lda.predict_test <- predict(wdbc_raw.lda, newdata = test_raw.df)
y<-wdbc_raw.lda.predict_test$class
wdbc_raw.lda.predict_test$x
table(y,test_raw.df$LayoffYesNo)
wdbc_raw.lda.predict.posteriors <- as.data.frame(wdbc_raw.lda.predict_test$posterior)
pred <- prediction(wdbc_raw.lda.predict.posteriors[,2], test_raw.df$LayoffYesNo)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
plot(wdbc_raw.lda, col = as.integer(train_raw.df$LayoffYesNo))



```

The accuracy provides a clear visualization of how well the model performs overall. 

The ROC curve offers deeper insights into the model's sensitivity and specificity

# CONCLUSION : 

Regression Summary-

The logistic regression model yielded the highest accuracy and demonstrated superior performance, as evidenced by its ROC curve. Logistic regression excelled in accurately predicting outcomes, particularly in distinguishing between troubled and non-troubled individuals based on their social media usage patterns.

# LEARNING :

* By examining historical data on layoffs, we can identify patterns and trends that may indicate factors contributing to layoffs in the technology industry.

* These models can be valuable tools for decision-making and risk management, allowing companies to proactively address potential workforce challenges and devise strategies to mitigate the impact of layoffs.

# TAKEAWAY:

The key takeaway from working on predicting the likelihood of future layoffs in tech companies based on historical data is the importance of leveraging data-driven insights for strategic decision-making and risk management. By harnessing the power of predictive analytics, organizations can anticipate and respond to workforce challenges more effectively, thereby enhancing resilience and adaptability in the dynamic technology industry landscape. 

